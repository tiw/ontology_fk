"""
æ€§èƒ½ä¼˜åŒ–æ¨¡å—

æä¾›ç¼“å­˜ã€ç´¢å¼•ã€è¿æ¥æ± ç­‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ï¼Œå®ç°æ¶æ„è®¾è®¡æ–‡æ¡£ä¸­å®šä¹‰çš„æ€§èƒ½ç­–ç•¥ã€‚
"""

import time
import threading
import hashlib
import json
from typing import Any, Dict, List, Optional, Callable, Union, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, OrderedDict
from functools import wraps
import weakref

# å¯¼å…¥éœ€è¦çš„æ¡†æ¶ç»„ä»¶
from .core import ObjectInstance

# ç¼“å­˜é…ç½®
@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    max_size: int = 1000
    ttl_seconds: int = 300  # 5åˆ†é’Ÿ
    cleanup_interval: int = 60  # 1åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
    enable_stats: bool = True


class LRUCache:
    """LRUç¼“å­˜å®ç°"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache: OrderedDict = OrderedDict()
        self.timestamps: Dict[str, float] = {}
        self.lock = threading.RLock()
        self.hits = 0
        self.misses = 0

    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜é¡¹æ˜¯å¦è¿‡æœŸ"""
        return time.time() - self.timestamps.get(key, 0) > self.ttl_seconds

    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸé¡¹"""
        current_time = time.time()
        expired_keys = [
            key for key, timestamp in self.timestamps.items()
            if current_time - timestamp > self.ttl_seconds
        ]

        for key in expired_keys:
            if key in self.cache:
                del self.cache[key]
            del self.timestamps[key]

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            self._cleanup_expired()

            if key not in self.cache:
                self.misses += 1
                return None

            if self._is_expired(key):
                del self.cache[key]
                del self.timestamps[key]
                self.misses += 1
                return None

            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            value = self.cache.pop(key)
            self.cache[key] = value
            self.hits += 1
            return value

    def put(self, key: str, value: Any):
        """å­˜å‚¨ç¼“å­˜å€¼"""
        with self.lock:
            current_time = time.time()

            # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°æ—¶é—´æˆ³
            if key in self.cache:
                self.cache[key] = value
                self.timestamps[key] = current_time
                return

            # æ¸…ç†è¿‡æœŸé¡¹
            self._cleanup_expired()

            # å¦‚æœè¾¾åˆ°æœ€å¤§å®¹é‡ï¼Œåˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
            if len(self.cache) >= self.max_size:
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
                del self.timestamps[oldest_key]

            self.cache[key] = value
            self.timestamps[key] = current_time

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()
            self.hits = 0
            self.misses = 0

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.hits + self.misses
            hit_rate = self.hits / total_requests if total_requests > 0 else 0

            return {
                "size": len(self.cache),
                "max_size": self.max_size,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": hit_rate,
                "ttl_seconds": self.ttl_seconds
            }


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, config: Optional[CacheConfig] = None):
        self.config = config or CacheConfig()
        self.caches: Dict[str, LRUCache] = {}
        self.lock = threading.RLock()

    def get_cache(self, name: str) -> LRUCache:
        """è·å–æˆ–åˆ›å»ºç¼“å­˜"""
        with self.lock:
            if name not in self.caches:
                self.caches[name] = LRUCache(
                    max_size=self.config.max_size,
                    ttl_seconds=self.config.ttl_seconds
                )
            return self.caches[name]

    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self.lock:
            for cache in self.caches.values():
                cache.clear()

    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {name: cache.get_stats() for name, cache in self.caches.items()}


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_manager = CacheManager()


def cached(cache_name: str = "default", key_func: Optional[Callable] = None, ttl_seconds: Optional[int] = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨å‡½æ•°åå’Œå‚æ•°çš„å“ˆå¸Œä½œä¸ºé”®
                key_data = f"{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}"
                cache_key = hashlib.md5(key_data.encode()).hexdigest()

            # è·å–ç¼“å­˜
            cache = _cache_manager.get_cache(cache_name)

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                return cached_result

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache.put(cache_key, result)

            return result

        # æ·»åŠ ç¼“å­˜ç®¡ç†æ–¹æ³•
        wrapper.cache_clear = lambda: _cache_manager.get_cache(cache_name).clear()
        wrapper.cache_stats = lambda: _cache_manager.get_cache(cache_name).get_stats()

        return wrapper
    return decorator


# ç´¢å¼•ç³»ç»Ÿ
@dataclass
class IndexDefinition:
    """ç´¢å¼•å®šä¹‰"""
    name: str
    property_name: str
    index_type: str = "hash"  # hash, btree, fulltext
    unique: bool = False
    case_sensitive: bool = True


class PropertyIndex:
    """å±æ€§ç´¢å¼•"""

    def __init__(self, definition: IndexDefinition):
        self.definition = definition
        self.index: Dict[Any, List[Any]] = defaultdict(list)
        self.lock = threading.RLock()

    def add(self, value: Any, object_id: Any):
        """æ·»åŠ ç´¢å¼•é¡¹"""
        with self.lock:
            # å¤„ç†å¤§å°å†™æ•æ„Ÿæ€§
            if isinstance(value, str) and not self.definition.case_sensitive:
                value = value.lower()

            if self.definition.unique:
                # å”¯ä¸€ç´¢å¼•
                if value in self.index:
                    raise ValueError(f"å”¯ä¸€ç´¢å¼•å†²çª: {self.definition.name} = {value}")
                self.index[value] = object_id
            else:
                # éå”¯ä¸€ç´¢å¼•
                self.index[value].append(object_id)

    def remove(self, value: Any, object_id: Any):
        """ç§»é™¤ç´¢å¼•é¡¹"""
        with self.lock:
            if isinstance(value, str) and not self.definition.case_sensitive:
                value = value.lower()

            if value in self.index:
                if self.definition.unique:
                    del self.index[value]
                else:
                    try:
                        self.index[value].remove(object_id)
                        if not self.index[value]:  # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œåˆ é™¤é”®
                            del self.index[value]
                    except ValueError:
                        pass  # å¯¹è±¡IDä¸å­˜åœ¨

    def find(self, value: Any) -> List[Any]:
        """æŸ¥æ‰¾ç´¢å¼•é¡¹"""
        with self.lock:
            if isinstance(value, str) and not self.definition.case_sensitive:
                value = value.lower()

            result = self.index.get(value, [])
            if self.definition.unique:
                return [result] if result else []
            return result.copy()

    def find_range(self, min_value: Any, max_value: Any) -> List[Any]:
        """èŒƒå›´æŸ¥æ‰¾ï¼ˆä»…æ”¯æŒå¯æ¯”è¾ƒçš„å€¼ï¼‰"""
        if self.definition.index_type != "btree":
            return []

        with self.lock:
            result = []
            for value, object_ids in self.index.items():
                if min_value <= value <= max_value:
                    result.extend(object_ids if not self.definition.unique else [object_ids])
            return result

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_values = len(self.index)
            total_objects = sum(
                len(object_ids) if not self.definition.unique else 1
                for object_ids in self.index.values()
            )

            return {
                "name": self.definition.name,
                "type": self.definition.index_type,
                "unique": self.definition.unique,
                "total_values": total_values,
                "total_objects": total_objects,
                "case_sensitive": self.definition.case_sensitive
            }


class IndexManager:
    """ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(self):
        self.indexes: Dict[str, PropertyIndex] = {}
        self.lock = threading.RLock()

    def create_index(self, definition: IndexDefinition) -> PropertyIndex:
        """åˆ›å»ºç´¢å¼•"""
        with self.lock:
            if definition.name in self.indexes:
                raise ValueError(f"ç´¢å¼•å·²å­˜åœ¨: {definition.name}")

            index = PropertyIndex(definition)
            self.indexes[definition.name] = index
            return index

    def get_index(self, name: str) -> Optional[PropertyIndex]:
        """è·å–ç´¢å¼•"""
        with self.lock:
            return self.indexes.get(name)

    def drop_index(self, name: str) -> bool:
        """åˆ é™¤ç´¢å¼•"""
        with self.lock:
            if name in self.indexes:
                del self.indexes[name]
                return True
            return False

    def list_indexes(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ç´¢å¼•åç§°"""
        with self.lock:
            return list(self.indexes.keys())

    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {name: index.get_stats() for name, index in self.indexes.items()}


# æŸ¥è¯¢ä¼˜åŒ–å™¨
class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, index_manager: IndexManager):
        self.index_manager = index_manager

    def optimize_filter_query(self, object_type: str, filters: Dict[str, Any]) -> Tuple[str, List[Any]]:
        """ä¼˜åŒ–è¿‡æ»¤æŸ¥è¯¢ï¼Œé€‰æ‹©æœ€ä½³ç´¢å¼•"""
        best_index = None
        best_index_name = None
        min_expected_results = float('inf')

        # æŸ¥æ‰¾é€‚ç”¨çš„ç´¢å¼•
        for index_name in self.index_manager.list_indexes():
            index = self.index_manager.get_index(index_name)

            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„ç´¢å¼•
            for property_name, value in filters.items():
                if index and index.definition.property_name == property_name:
                    # ä¼°ç®—ç»“æœæ•°é‡
                    estimated_results = self._estimate_selectivity(index, value)

                    if estimated_results < min_expected_results:
                        min_expected_results = estimated_results
                        best_index = index
                        best_index_name = index_name

        if best_index:
            return best_index_name, best_index.find(filters.get(best_index.definition.property_name))

        return None, []

    def _estimate_selectivity(self, index: PropertyIndex, value: Any) -> int:
        """ä¼°ç®—ç´¢å¼•é€‰æ‹©æ€§"""
        stats = index.get_stats()
        total_objects = stats["total_objects"]

        if total_objects == 0:
            return 0

        # ç®€å•å¯å‘å¼ï¼šå¦‚æœå€¼åœ¨ç´¢å¼•ä¸­ï¼Œå‡è®¾è¿”å›10%çš„æ•°æ®
        if value in index.index:
            return max(1, total_objects // 10)

        # å¦‚æœå€¼ä¸åœ¨ç´¢å¼•ä¸­ï¼Œå‡è®¾è¿”å›0
        return 0


# è¿æ¥æ± 
@dataclass
class ConnectionConfig:
    """è¿æ¥é…ç½®"""
    max_connections: int = 10
    min_connections: int = 2
    max_idle_time: int = 300  # 5åˆ†é’Ÿ
    connection_timeout: int = 30  # 30ç§’
    health_check_interval: int = 60  # 1åˆ†é’Ÿ


class ConnectionPool:
    """é€šç”¨è¿æ¥æ± """

    def __init__(self, config: ConnectionConfig, connection_factory: Callable):
        self.config = config
        self.connection_factory = connection_factory

        self.available_connections: List[Any] = []
        self.used_connections: weakref.WeakSet = weakref.WeakSet()
        self.connection_timestamps: Dict[int, float] = {}

        self.lock = threading.RLock()
        self.last_health_check = time.time()

        # åˆå§‹åŒ–æœ€å°è¿æ¥æ•°
        self._initialize_pool()

    def _initialize_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        for _ in range(self.config.min_connections):
            try:
                conn = self.connection_factory()
                self.available_connections.append(conn)
            except Exception as e:
                print(f"åˆå§‹åŒ–è¿æ¥å¤±è´¥: {e}")

    def _create_connection(self) -> Any:
        """åˆ›å»ºæ–°è¿æ¥"""
        return self.connection_factory()

    def _is_connection_healthy(self, connection: Any) -> bool:
        """æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€"""
        try:
            # ç®€å•çš„å¥åº·æ£€æŸ¥ï¼ˆéœ€è¦æ ¹æ®å…·ä½“è¿æ¥ç±»å‹å®ç°ï¼‰
            return hasattr(connection, 'ping') or connection is not None
        except:
            return False

    def _cleanup_idle_connections(self):
        """æ¸…ç†ç©ºé—²è¿æ¥"""
        current_time = time.time()
        idle_connections = []

        for i, conn in enumerate(self.available_connections):
            conn_id = id(conn)
            last_used = self.connection_timestamps.get(conn_id, 0)

            if current_time - last_used > self.config.max_idle_time:
                idle_connections.append(i)

        # ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•é—®é¢˜
        for i in reversed(idle_connections):
            conn = self.available_connections.pop(i)
            try:
                # å°è¯•å…³é—­è¿æ¥
                if hasattr(conn, 'close'):
                    conn.close()
            except:
                pass

    def _health_check(self):
        """å¥åº·æ£€æŸ¥"""
        current_time = time.time()

        # å¦‚æœè·ç¦»ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´ä¸è¶³é—´éš”æ—¶é—´ï¼Œè·³è¿‡
        if current_time - self.last_health_check < self.config.health_check_interval:
            return

        # æ¸…ç†ç©ºé—²è¿æ¥
        self._cleanup_idle_connections()

        # æ£€æŸ¥å¯ç”¨è¿æ¥å¥åº·çŠ¶æ€
        healthy_connections = []
        for conn in self.available_connections:
            if self._is_connection_healthy(conn):
                healthy_connections.append(conn)
            else:
                # ç§»é™¤ä¸å¥åº·çš„è¿æ¥
                conn_id = id(conn)
                self.connection_timestamps.pop(conn_id, None)

        self.available_connections = healthy_connections

        # ç¡®ä¿æœ€å°è¿æ¥æ•°
        while len(self.available_connections) < self.config.min_connections:
            try:
                conn = self._create_connection()
                self.available_connections.append(conn)
            except Exception as e:
                print(f"åˆ›å»ºè¿æ¥å¤±è´¥: {e}")
                break

        self.last_health_check = current_time

    def get_connection(self) -> Any:
        """è·å–è¿æ¥"""
        with self.lock:
            self._health_check()

            # å°è¯•ä»å¯ç”¨è¿æ¥ä¸­è·å–
            if self.available_connections:
                conn = self.available_connections.pop()
                self.used_connections.add(conn)
                self.connection_timestamps[id(conn)] = time.time()
                return conn

            # å¦‚æœæ²¡æœ‰å¯ç”¨è¿æ¥ä¸”æœªè¾¾åˆ°æœ€å¤§è¿æ¥æ•°ï¼Œåˆ›å»ºæ–°è¿æ¥
            if len(self.used_connections) < self.config.max_connections:
                try:
                    conn = self._create_connection()
                    self.used_connections.add(conn)
                    self.connection_timestamps[id(conn)] = time.time()
                    return conn
                except Exception as e:
                    raise RuntimeError(f"åˆ›å»ºè¿æ¥å¤±è´¥: {e}")

            # è¾¾åˆ°æœ€å¤§è¿æ¥æ•°ï¼Œç­‰å¾…å¯ç”¨è¿æ¥
            raise RuntimeError("è¿æ¥æ± å·²æ»¡ï¼Œæ— æ³•è·å–è¿æ¥")

    def return_connection(self, connection: Any):
        """å½’è¿˜è¿æ¥"""
        with self.lock:
            if connection in self.used_connections:
                self.used_connections.discard(connection)

                if self._is_connection_healthy(connection):
                    self.available_connections.append(connection)
                    self.connection_timestamps[id(connection)] = time.time()
                else:
                    # è¿æ¥ä¸å¥åº·ï¼Œå…³é—­å®ƒ
                    try:
                        if hasattr(connection, 'close'):
                            connection.close()
                    except:
                        pass
                    conn_id = id(connection)
                    self.connection_timestamps.pop(conn_id, None)

    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self.lock:
            all_connections = self.available_connections + list(self.used_connections)

            for conn in all_connections:
                try:
                    if hasattr(conn, 'close'):
                        conn.close()
                except:
                    pass

            self.available_connections.clear()
            self.used_connections.clear()
            self.connection_timestamps.clear()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "available_connections": len(self.available_connections),
                "used_connections": len(self.used_connections),
                "max_connections": self.config.max_connections,
                "min_connections": self.config.min_connections,
                "connection_timestamps": len(self.connection_timestamps)
            }


# æ€§èƒ½ç›‘æ§
@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    operation_count: int = 0
    total_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    error_count: int = 0

    def update(self, execution_time: float, success: bool = True):
        """æ›´æ–°æŒ‡æ ‡"""
        self.operation_count += 1
        self.total_time += execution_time
        self.min_time = min(self.min_time, execution_time)
        self.max_time = max(self.max_time, execution_time)

        if not success:
            self.error_count += 1

    @property
    def avg_time(self) -> float:
        """å¹³å‡æ—¶é—´"""
        return self.total_time / self.operation_count if self.operation_count > 0 else 0.0

    @property
    def error_rate(self) -> float:
        """é”™è¯¯ç‡"""
        return self.error_count / self.operation_count if self.operation_count > 0 else 0.0


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics: Dict[str, PerformanceMetrics] = {}
        self.lock = threading.RLock()

    def record_operation(self, operation_name: str, execution_time: float, success: bool = True):
        """è®°å½•æ“ä½œ"""
        with self.lock:
            if operation_name not in self.metrics:
                self.metrics[operation_name] = PerformanceMetrics()

            self.metrics[operation_name].update(execution_time, success)

    def get_metrics(self, operation_name: str) -> Optional[PerformanceMetrics]:
        """è·å–æ“ä½œæŒ‡æ ‡"""
        with self.lock:
            return self.metrics.get(operation_name)

    def get_all_metrics(self) -> Dict[str, PerformanceMetrics]:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        with self.lock:
            return self.metrics.copy()

    def clear_metrics(self, operation_name: Optional[str] = None):
        """æ¸…é™¤æŒ‡æ ‡"""
        with self.lock:
            if operation_name:
                self.metrics.pop(operation_name, None)
            else:
                self.metrics.clear()


def performance_monitored(operation_name: Optional[str] = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        name = operation_name or f"{func.__module__}.{func.__name__}"

        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            success = False

            try:
                result = func(*args, **kwargs)
                success = True
                return result
            finally:
                execution_time = time.time() - start_time
                _performance_monitor.record_operation(name, execution_time, success)

        return wrapper
    return decorator


# å…¨å±€å®ä¾‹
_cache_manager = CacheManager()
_performance_monitor = PerformanceMonitor()


def get_cache_manager() -> CacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨"""
    return _cache_manager


def get_performance_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å™¨"""
    return _performance_monitor


# æ€§èƒ½ä¼˜åŒ–å»ºè®®
class PerformanceAdvisor:
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®å™¨"""

    def __init__(self):
        self.cache_manager = get_cache_manager()
        self.performance_monitor = get_performance_monitor()

    def analyze_performance(self) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½å¹¶æä¾›å»ºè®®"""
        recommendations = []

        # åˆ†æç¼“å­˜æ€§èƒ½
        cache_stats = self.cache_manager.get_all_stats()
        for cache_name, stats in cache_stats.items():
            if stats["hit_rate"] < 0.7:  # å‘½ä¸­ç‡ä½äº70%
                recommendations.append({
                    "type": "cache",
                    "severity": "medium",
                    "component": cache_name,
                    "issue": f"ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {stats['hit_rate']:.2%}",
                    "recommendation": "è€ƒè™‘å¢åŠ ç¼“å­˜å¤§å°æˆ–è°ƒæ•´TTLæ—¶é—´",
                    "current_stats": stats
                })

        # åˆ†ææ“ä½œæ€§èƒ½
        performance_metrics = self.performance_monitor.get_all_metrics()
        for operation_name, metrics in performance_metrics.items():
            if metrics.avg_time > 0.1:  # å¹³å‡å“åº”æ—¶é—´è¶…è¿‡100ms
                recommendations.append({
                    "type": "performance",
                    "severity": "high",
                    "component": operation_name,
                    "issue": f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {metrics.avg_time:.3f}s",
                    "recommendation": "è€ƒè™‘æ·»åŠ ç¼“å­˜ã€ä¼˜åŒ–æŸ¥è¯¢æˆ–ä½¿ç”¨ç´¢å¼•",
                    "current_metrics": {
                        "avg_time": metrics.avg_time,
                        "operation_count": metrics.operation_count,
                        "error_rate": metrics.error_rate
                    }
                })

            if metrics.error_rate > 0.05:  # é”™è¯¯ç‡è¶…è¿‡5%
                recommendations.append({
                    "type": "reliability",
                    "severity": "high",
                    "component": operation_name,
                    "issue": f"é”™è¯¯ç‡è¿‡é«˜: {metrics.error_rate:.2%}",
                    "recommendation": "æ£€æŸ¥é”™è¯¯å¤„ç†é€»è¾‘å’Œé‡è¯•æœºåˆ¶",
                    "current_metrics": {
                        "error_rate": metrics.error_rate,
                        "error_count": metrics.error_count,
                        "operation_count": metrics.operation_count
                    }
                })

        return recommendations

    def generate_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        recommendations = self.analyze_performance()

        if not recommendations:
            return "âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–å»ºè®®"

        report = ["ğŸ” æ€§èƒ½ä¼˜åŒ–å»ºè®®æŠ¥å‘Š", "=" * 40, ""]

        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        by_severity = defaultdict(list)
        for rec in recommendations:
            by_severity[rec["severity"]].append(rec)

        for severity in ["high", "medium", "low"]:
            if severity in by_severity:
                severity_text = {"high": "ğŸ”´ é«˜ä¼˜å…ˆçº§", "medium": "ğŸŸ¡ ä¸­ä¼˜å…ˆçº§", "low": "ğŸŸ¢ ä½ä¼˜å…ˆçº§"}
                report.append(f"{severity_text[severity]}:")

                for rec in by_severity[severity]:
                    report.append(f"  â€¢ ç»„ä»¶: {rec['component']}")
                    report.append(f"    é—®é¢˜: {rec['issue']}")
                    report.append(f"    å»ºè®®: {rec['recommendation']}")
                    report.append("")

        return "\n".join(report)


# ä¸ç°æœ‰ optimized_core.py é›†æˆçš„é€‚é…å™¨
class PerformanceOptimizerAdapter:
    """æ€§èƒ½ä¼˜åŒ–é€‚é…å™¨ï¼Œé›†æˆæ–°çš„æ€§èƒ½æ¨¡å—åˆ°ç°æœ‰ç³»ç»Ÿ"""

    def __init__(self, ontology):
        self.ontology = ontology
        self.advisor = PerformanceAdvisor()

    def install_optimizations(self):
        """å®‰è£…æ€§èƒ½ä¼˜åŒ–"""
        # ä¸ºç°æœ‰å¯¹è±¡åˆ›å»ºç¼“å­˜
        if hasattr(self.ontology, '_object_store'):
            for object_type_name, objects in self.ontology._object_store.items():
                # ä¸ºæ¯ä¸ªå¯¹è±¡ç±»å‹åˆ›å»ºç¼“å­˜
                cache_name = f"objects_{object_type_name}"
                self.ontology._cache_manager.get_cache(cache_name)

        # ä¸ºå¸¸ç”¨æŸ¥è¯¢åˆ›å»ºç´¢å¼•
        if hasattr(self.ontology, 'object_types'):
            for object_type_name, object_type in self.ontology.object_types.items():
                # ä¸ºä¸»é”®åˆ›å»ºç´¢å¼•
                if object_type.primary_key:
                    self.create_optimized_index(object_type_name, object_type.primary_key, unique=True)

                # ä¸ºå¸¸ç”¨å±æ€§åˆ›å»ºç´¢å¼•
                common_properties = ['name', 'email', 'status', 'type']
                for prop in common_properties:
                    if prop in object_type.properties:
                        self.create_optimized_index(object_type_name, prop)

    def create_optimized_index(self, object_type_name: str, property_name: str, unique: bool = False):
        """åˆ›å»ºä¼˜åŒ–çš„ç´¢å¼•"""
        if hasattr(self.ontology, 'index_manager'):
            return self.ontology.index_manager.create_property_index(object_type_name, property_name)
        return False

    def get_optimization_recommendations(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        analysis = self.advisor.analyze_performance()

        for rec in analysis:
            if rec["severity"] == "high":
                recommendations.append(f"ç´§æ€¥: {rec['issue']}")
            elif rec["severity"] == "medium":
                recommendations.append(f"å»ºè®®: {rec['issue']}")

        return recommendations

    def apply_auto_optimizations(self):
        """åº”ç”¨è‡ªåŠ¨ä¼˜åŒ–"""
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        if hasattr(self.ontology, '_cache_manager'):
            self.ontology._cache_manager.clear_all()

        # é‡å»ºç´¢å¼•
        if hasattr(self.ontology, 'index_manager'):
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç´¢å¼•é‡å»ºé€»è¾‘
            pass

        # ä¼˜åŒ–å¯¹è±¡é›†åˆ
        if hasattr(self.ontology, '_object_store'):
            for object_type_name in self.ontology._object_store:
                objects = self.ontology.get_objects_of_type(object_type_name)
                if hasattr(objects, 'invalidate_cache'):
                    objects.invalidate_cache()


# æ‰¹é‡æ“ä½œä¼˜åŒ–
@dataclass
class BatchConfig:
    """æ‰¹é‡æ“ä½œé…ç½®"""
    batch_size: int = 1000
    max_memory_usage: int = 100 * 1024 * 1024  # 100MB
    enable_parallel: bool = True
    parallel_workers: int = 4


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, config: BatchConfig = None):
        self.config = config or BatchConfig()

    @performance_monitored("batch_add_objects")
    def batch_add_objects(self, ontology, objects: List[ObjectInstance]) -> Dict[str, Any]:
        """æ‰¹é‡æ·»åŠ å¯¹è±¡"""
        start_time = time.time()
        success_count = 0
        error_count = 0

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(objects), self.config.batch_size):
            batch = objects[i:i + self.config.batch_size]

            try:
                for obj in batch:
                    ontology.add_object(obj)
                    success_count += 1
            except Exception as e:
                error_count += len(batch)
                print(f"æ‰¹é‡æ·»åŠ å‡ºé”™: {e}")

        execution_time = time.time() - start_time

        return {
            "total_objects": len(objects),
            "success_count": success_count,
            "error_count": error_count,
            "execution_time": execution_time,
            "throughput": len(objects) / execution_time if execution_time > 0 else 0
        }

    @performance_monitored("batch_query")
    def batch_query(self, ontology, object_type_name: str, queries: List[Dict[str, Any]]) -> List[ObjectInstance]:
        """æ‰¹é‡æŸ¥è¯¢"""
        results = []

        # å¦‚æœæœ‰ç´¢å¼•ï¼Œä¼˜å…ˆä½¿ç”¨ç´¢å¼•æŸ¥è¯¢
        if hasattr(ontology, 'index_manager'):
            for query in queries:
                # å°è¯•ä½¿ç”¨ç´¢å¼•
                if len(query) == 1:  # å•æ¡ä»¶æŸ¥è¯¢
                    prop_name, prop_value = next(iter(query.items()))
                    index_key = f"{object_type_name}.{prop_name}"

                    if hasattr(ontology.index_manager, 'property_index') and index_key in ontology.index_manager.property_index:
                        matching_pks = ontology.index_manager.query_with_index(object_type_name, query)
                        matching_objects = ontology.index_manager.find_objects_by_primary_key(object_type_name, list(matching_pks))
                        results.extend(matching_objects)
                        continue

                # å›é€€åˆ°æ ‡å‡†æŸ¥è¯¢
                objects = ontology.get_objects_of_type(object_type_name)
                filtered = objects
                for prop_name, prop_value in query.items():
                    filtered = filtered.filter(prop_name, prop_value)
                results.extend(filtered.all())
        else:
            # æ²¡æœ‰ç´¢å¼•ï¼Œä½¿ç”¨æ ‡å‡†æŸ¥è¯¢
            objects = ontology.get_objects_of_type(object_type_name)
            for query in queries:
                filtered = objects
                for prop_name, prop_value in query.items():
                    filtered = filtered.filter(prop_name, prop_value)
                results.extend(filtered.all())

        return results


# å†…å­˜ä¼˜åŒ–å™¨
class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""

    def __init__(self, ontology):
        self.ontology = ontology

    def analyze_memory_usage(self) -> Dict[str, Any]:
        """åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import sys

        memory_stats = {}

        # åˆ†æå¯¹è±¡å­˜å‚¨
        if hasattr(self.ontology, '_object_store'):
            total_objects = 0
            total_memory = 0

            for object_type_name, objects in self.ontology._object_store.items():
                object_count = len(objects)
                object_memory = sum(sys.getsizeof(obj) for obj in objects.values())

                total_objects += object_count
                total_memory += object_memory

                memory_stats[object_type_name] = {
                    "object_count": object_count,
                    "memory_usage": object_memory,
                    "avg_memory_per_object": object_memory / object_count if object_count > 0 else 0
                }

            memory_stats["summary"] = {
                "total_objects": total_objects,
                "total_memory": total_memory,
                "avg_memory_per_object": total_memory / total_objects if total_objects > 0 else 0
            }

        # åˆ†æç¼“å­˜ä½¿ç”¨
        if hasattr(self.ontology, '_cache_manager'):
            cache_stats = self.ontology._cache_manager.get_all_stats()
            memory_stats["cache_summary"] = cache_stats

        return memory_stats

    def optimize_memory_usage(self) -> List[str]:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        optimizations = []

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        if hasattr(self.ontology, '_cache_manager'):
            old_cache_size = sum(len(cache.cache) for cache in self.ontology._cache_manager.caches.values())
            self.ontology._cache_manager.clear_all()
            optimizations.append(f"æ¸…ç†ç¼“å­˜ï¼Œé‡Šæ”¾äº† {old_cache_size} ä¸ªç¼“å­˜é¡¹")

        # æ¸…ç†å¯¹è±¡ç¼“å­˜
        if hasattr(self.ontology, '_object_store'):
            cleared_objects = 0
            for objects in self.ontology._object_store.values():
                for obj in objects.values():
                    if hasattr(obj, '_derived_properties_cache'):
                        cache_size = len(obj._derived_properties_cache)
                        obj._derived_properties_cache.clear()
                        obj._cache_timestamps.clear()
                        cleared_objects += cache_size

            optimizations.append(f"æ¸…ç†äº† {cleared_objects} ä¸ªå¯¹è±¡çš„æ´¾ç”Ÿå±æ€§ç¼“å­˜")

        return optimizations

    def suggest_memory_optimizations(self) -> List[str]:
        """å»ºè®®å†…å­˜ä¼˜åŒ–"""
        suggestions = []
        memory_stats = self.analyze_memory_usage()

        if "summary" in memory_stats:
            total_memory = memory_stats["summary"]["total_memory"]
            if total_memory > 50 * 1024 * 1024:  # è¶…è¿‡50MB
                suggestions.append("å†…å­˜ä½¿ç”¨é‡è¾ƒé«˜ï¼Œè€ƒè™‘ä½¿ç”¨å¯¹è±¡æ± æˆ–å»¶è¿ŸåŠ è½½")

        # æ£€æŸ¥å¤§å¯¹è±¡
        for object_type_name, stats in memory_stats.items():
            if object_type_name == "summary" or object_type_name == "cache_summary":
                continue

            avg_memory = stats["avg_memory_per_object"]
            if avg_memory > 1024:  # å¹³å‡æ¯ä¸ªå¯¹è±¡è¶…è¿‡1KB
                suggestions.append(f"å¯¹è±¡ç±»å‹ {object_type_name} å¹³å‡å†…å­˜ä½¿ç”¨è¾ƒé«˜ ({avg_memory} bytes)ï¼Œè€ƒè™‘ä¼˜åŒ–å±æ€§å­˜å‚¨")

        return suggestions