# ç”Ÿäº§ç¯å¢ƒç›‘æ§æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—æä¾› Ontology Framework åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´ç›‘æ§è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç³»ç»Ÿç›‘æ§ã€åº”ç”¨ç›‘æ§ã€æ—¥å¿—ç®¡ç†ã€å‘Šè­¦é…ç½®å’Œæ•…éšœæ’æŸ¥çš„æœ€ä½³å®è·µã€‚

## ç›®å½•

1. [ç›‘æ§æ¶æ„](#ç›‘æ§æ¶æ„)
2. [åŸºç¡€è®¾æ–½ç›‘æ§](#åŸºç¡€è®¾æ–½ç›‘æ§)
3. [åº”ç”¨æ€§èƒ½ç›‘æ§](#åº”ç”¨æ€§èƒ½ç›‘æ§)
4. [æ—¥å¿—ç®¡ç†](#æ—¥å¿—ç®¡ç†)
5. [å‘Šè­¦é…ç½®](#å‘Šè­¦é…ç½®)
6. [ç›‘æ§å·¥å…·é›†æˆ](#ç›‘æ§å·¥å…·é›†æˆ)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
8. [ç›‘æ§æœ€ä½³å®è·µ](#ç›‘æ§æœ€ä½³å®è·µ)

## ç›‘æ§æ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç›‘æ§æ•°æ®æº"
        APP[Ontology Framework åº”ç”¨]
        SYS[ç³»ç»ŸæŒ‡æ ‡]
        LOGS[æ—¥å¿—æ–‡ä»¶]
        METRICS[åº”ç”¨æŒ‡æ ‡]
    end

    subgraph "æ•°æ®æ”¶é›†å±‚"
        PROM[Prometheus]
        FLUENT[Fluentd/Logstash]
        NODE[Node Exporter]
        APP_MON[åº”ç”¨ç›‘æ§Agent]
    end

    subgraph "æ•°æ®å­˜å‚¨å±‚"
        TSDB[æ—¶åºæ•°æ®åº“]
        ES[Elasticsearch]
        GRAFANA_DB[Grafana æ•°æ®åº“]
    end

    subgraph "å¯è§†åŒ–å±‚"
        GRAFANA[Grafana ä»ªè¡¨æ¿]
        KIBANA[Kibana æ—¥å¿—åˆ†æ]
        ALERTMGR[AlertManager]
    end

    subgraph "å‘Šè­¦å±‚"
        SLACK[Slack é€šçŸ¥]
        EMAIL[é‚®ä»¶é€šçŸ¥]
        WEBHOOK[Webhook é€šçŸ¥]
        SMS[çŸ­ä¿¡é€šçŸ¥]
    end

    APP --> PROM
    APP --> FLUENT
    APP --> APP_MON
    SYS --> NODE
    LOGS --> FLUENT
    METRICS --> PROM

    PROM --> TSDB
    FLUENT --> ES
    APP_MON --> TSDB

    TSDB --> GRAFANA
    ES --> KIBANA
    TSDB --> ALERTMGR

    GRAFANA --> SLACK
    KIBANA --> SLACK
    ALERTMGR --> SLACK
    ALERTMGR --> EMAIL
    ALERTMGR --> WEBHOOK
    ALERTMGR --> SMS
```

### ç›‘æ§å±‚æ¬¡

```mermaid
graph LR
    L1[åŸºç¡€è®¾æ–½å±‚] --> L2[å®¹å™¨å±‚]
    L2 --> L3[åº”ç”¨å±‚]
    L3 --> L4[ä¸šåŠ¡å±‚]
    L4 --> L5[ç”¨æˆ·ä½“éªŒå±‚]
```

## åŸºç¡€è®¾æ–½ç›‘æ§

### 1. æœåŠ¡å™¨ç›‘æ§

#### æ ¸å¿ƒæŒ‡æ ‡

**ç³»ç»Ÿèµ„æºæŒ‡æ ‡**
```yaml
ç³»ç»Ÿç›‘æ§æŒ‡æ ‡:
  CPU:
    - ä½¿ç”¨ç‡ (user, system, idle, iowait)
    - è´Ÿè½½å¹³å‡å€¼ (1m, 5m, 15m)
    - ä¸Šä¸‹æ–‡åˆ‡æ¢ç‡
    - CPU é¥¥é¥¿æ—¶é—´

  å†…å­˜:
    - æ€»å†…å­˜ã€å·²ç”¨å†…å­˜ã€å¯ç”¨å†…å­˜
    - å†…å­˜ä½¿ç”¨ç‡
    - äº¤æ¢ç©ºé—´ä½¿ç”¨æƒ…å†µ
    - ç¼“å­˜å’Œç¼“å†²åŒºä½¿ç”¨

  ç£ç›˜:
    - ç£ç›˜ä½¿ç”¨ç‡
    - I/O è¯»å†™é€Ÿç‡
    - I/O ç­‰å¾…æ—¶é—´
    - ç£ç›˜é˜Ÿåˆ—é•¿åº¦

  ç½‘ç»œ:
    - ç½‘ç»œååé‡ (bytes/s)
    - ç½‘ç»œåŒ…é€Ÿç‡ (packets/s)
    - ç½‘ç»œé”™è¯¯ç‡
    - TCP è¿æ¥çŠ¶æ€
```

**Prometheus é…ç½®ç¤ºä¾‹**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # Node Exporter - ç³»ç»ŸæŒ‡æ ‡
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

  # åº”ç”¨æŒ‡æ ‡
  - job_name: 'ontology-framework'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/metrics'
    scrape_interval: 10s

rule_files:
  - "alert_rules.yml"
```

### 2. å®¹å™¨ç›‘æ§ (Docker/Kubernetes)

#### Kubernetes ç›‘æ§é…ç½®

```yaml
# kubernetes-monitoring.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s

    scrape_configs:
      # Kubernetes API Server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Kubelet
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Ontology Framework Pods
      - job_name: 'ontology-framework-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: ontology-framework
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
```

#### Docker ç›‘æ§

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning

  node-exporter:
    image: prom/node-exporter:latest
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

volumes:
  prometheus_data:
  grafana_data:
```

## åº”ç”¨æ€§èƒ½ç›‘æ§

### 1. åº”ç”¨æŒ‡æ ‡ç›‘æ§

#### å…³é”®ä¸šåŠ¡æŒ‡æ ‡

```python
# åº”ç”¨æŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# ä¸šåŠ¡æŒ‡æ ‡
object_operations_total = Counter(
    'ontology_object_operations_total',
    'Total number of object operations',
    ['operation', 'object_type', 'status']
)

query_duration = Histogram(
    'ontology_query_duration_seconds',
    'Query execution duration',
    ['query_type', 'object_type'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

active_sessions = Gauge(
    'ontology_active_sessions',
    'Number of active user sessions'
)

cache_hit_rate = Gauge(
    'ontology_cache_hit_rate',
    'Cache hit rate',
    ['cache_name']
)

memory_usage = Gauge(
    'ontology_memory_usage_bytes',
    'Memory usage by component',
    ['component']
)

# æ€§èƒ½æŒ‡æ ‡è£…é¥°å™¨
def monitor_performance(operation_name: str, object_type: str = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            status = 'success'

            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                status = 'error'
                raise
            finally:
                duration = time.time() - start_time
                query_duration.labels(
                    query_type=operation_name,
                    object_type=object_type or 'unknown'
                ).observe(duration)

                object_operations_total.labels(
                    operation=operation_name,
                    object_type=object_type or 'unknown',
                    status=status
                ).inc()

        return wrapper
    return decorator
```

#### è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡

```python
class BusinessMetrics:
    """ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.daily_active_users = Gauge('ontology_daily_active_users', 'Daily active users')
        self.object_count_by_type = Gauge('ontology_objects_by_type', 'Object count by type', ['type'])
        self.api_request_rate = Counter('ontology_api_requests_total', 'API request rate', ['endpoint', 'method', 'status'])
        self.data_integrity_score = Gauge('ontology_data_integrity_score', 'Data integrity score', ['validation_type'])

    def record_user_activity(self, user_id: str):
        """è®°å½•ç”¨æˆ·æ´»åŠ¨"""
        self.daily_active_users.inc()

    def update_object_count(self, object_type: str, count: int):
        """æ›´æ–°å¯¹è±¡æ•°é‡"""
        self.object_count_by_type.labels(type=object_type).set(count)

    def record_api_request(self, endpoint: str, method: str, status: int):
        """è®°å½•APIè¯·æ±‚"""
        self.api_request_rates.labels(
            endpoint=endpoint,
            method=method,
            status=str(status)
        ).inc()

    def update_integrity_score(self, validation_type: str, score: float):
        """æ›´æ–°æ•°æ®å®Œæ•´æ€§åˆ†æ•°"""
        self.data_integrity_score.labels(validation_type=validation_type).set(score)

# é›†æˆåˆ°åº”ç”¨ä¸­
business_metrics = BusinessMetrics()

@monitor_performance('create_object', 'employee')
def create_employee_object(employee_data):
    # ä¸šåŠ¡é€»è¾‘
    result = ontology.add_object(employee_instance)

    # æ›´æ–°ä¸šåŠ¡æŒ‡æ ‡
    business_metrics.update_object_count('employee',
        len(ontology.get_objects_of_type('employee').all()))

    return result
```

### 2. ç”¨æˆ·ä½“éªŒç›‘æ§

#### å‰ç«¯æ€§èƒ½ç›‘æ§

```javascript
// å‰ç«¯æ€§èƒ½ç›‘æ§è„šæœ¬
class PerformanceMonitor {
    constructor() {
        this.metrics = {
            pageLoadTime: 0,
            apiResponseTime: {},
            errorCount: 0,
            userInteractions: 0
        };
        this.init();
    }

    init() {
        // é¡µé¢åŠ è½½æ€§èƒ½
        window.addEventListener('load', () => {
            const loadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;
            this.metrics.pageLoadTime = loadTime;
            this.sendMetric('page_load_time', loadTime);
        });

        // API è¯·æ±‚ç›‘æ§
        this.interceptFetch();

        // é”™è¯¯ç›‘æ§
        window.addEventListener('error', this.handleError.bind(this));
        window.addEventListener('unhandledrejection', this.handlePromiseRejection.bind(this));
    }

    interceptFetch() {
        const originalFetch = window.fetch;
        window.fetch = async (...args) => {
            const start = performance.now();
            const url = args[0];

            try {
                const response = await originalFetch(...args);
                const duration = performance.now() - start;

                this.sendMetric('api_response_time', duration, {
                    url: url,
                    status: response.status,
                    method: args[1]?.method || 'GET'
                });

                return response;
            } catch (error) {
                const duration = performance.now() - start;
                this.sendMetric('api_error', duration, {
                    url: url,
                    error: error.message
                });
                throw error;
            }
        };
    }

    handleError(event) {
        this.metrics.errorCount++;
        this.sendMetric('javascript_error', 1, {
            message: event.message,
            filename: event.filename,
            lineno: event.lineno
        });
    }

    handlePromiseRejection(event) {
        this.metrics.errorCount++;
        this.sendMetric('promise_rejection', 1, {
            reason: event.reason
        });
    }

    trackUserInteraction(action, details = {}) {
        this.metrics.userInteractions++;
        this.sendMetric('user_interaction', 1, {
            action: action,
            ...details
        });
    }

    sendMetric(name, value, tags = {}) {
        // å‘é€åˆ°åç«¯ç›‘æ§ç«¯ç‚¹
        fetch('/api/metrics', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                name: name,
                value: value,
                timestamp: Date.now(),
                tags: tags,
                sessionId: this.getSessionId(),
                userAgent: navigator.userAgent
            })
        }).catch(console.error);
    }

    getSessionId() {
        let sessionId = sessionStorage.getItem('monitoring_session_id');
        if (!sessionId) {
            sessionId = 'session_' + Date.now() + '_' + Math.random().toString(36).substr(2, 9);
            sessionStorage.setItem('monitoring_session_id', sessionId);
        }
        return sessionId;
    }
}

// åˆå§‹åŒ–ç›‘æ§
const performanceMonitor = new PerformanceMonitor();

// ç”¨æˆ·äº¤äº’è·Ÿè¸ªç¤ºä¾‹
document.getElementById('submit-button')?.addEventListener('click', () => {
    performanceMonitor.trackUserInteraction('button_click', {
        element: 'submit-button',
        page: window.location.pathname
    });
});
```

## æ—¥å¿—ç®¡ç†

### 1. ç»“æ„åŒ–æ—¥å¿—é…ç½®

#### Logstash é…ç½®

```ruby
# logstash/pipeline.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # è§£æ JSON æ—¥å¿—
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }

  # è§£ææ—¶é—´æˆ³
  date {
    match => [ "timestamp", "ISO8601" ]
  }

  # æ·»åŠ ç¯å¢ƒä¿¡æ¯
  mutate {
    add_field => { "environment" => "${ENVIRONMENT:development}" }
    add_field => { "service" => "ontology-framework" }
  }

  # è§£ææ—¥å¿—çº§åˆ«
  grok {
    match => {
      "level" => "%{LOGLEVEL:log_level}"
    }
  }

  # æ•æ„Ÿä¿¡æ¯è„±æ•
  mutate {
    gsub => [
      "message", "password=\w+", "password=***",
      "message", "token=\w+", "token=***",
      "message", "secret=\w+", "secret=***"
    ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ontology-logs-%{+YYYY.MM.dd}"
  }

  # é”™è¯¯æ—¥å¿—å•ç‹¬å­˜å‚¨
  if [log_level] == "ERROR" or [log_level] == "FATAL" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ontology-error-logs-%{+YYYY.MM.dd}"
    }
  }
}
```

#### Fluentd é…ç½®

```ruby
# fluent.conf
<source>
  @type tail
  path /var/log/ontology-framework/*.log
  pos_file /var/log/fluentd/ontology-framework.log.pos
  tag ontology.*
  format json
  time_key timestamp
  time_format %Y-%m-%dT%H:%M:%S%.NZ
</source>

<filter ontology.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
    service ontology-framework
  </record>
</filter>

<match ontology.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name ontology-logs-%Y%m%d
  type_name _doc

  # é”™è¯¯æ—¥å¿—å•ç‹¬å¤„ç†
  <buffer tag, time>
    @type file
    path /var/log/fluentd/buffer
    timekey_wait 10m
    timekey 1d
  </buffer>

  # æ ¹æ®æ—¥å¿—çº§åˆ«è·¯ç”±åˆ°ä¸åŒç´¢å¼•
  <secondary>
    @type rewrite_tag_filter
    rewriterule1 level ERROR ontology.error
    rewriterule2 level FATAL ontology.fatal
  </secondary>
</match>

<match ontology.error>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name ontology-error-logs-%Y%m%d
  type_name _doc
</match>
```

### 2. æ—¥å¿—èšåˆå’Œåˆ†æ

#### Kibana ä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "Ontology Framework ç›‘æ§ä»ªè¡¨æ¿",
    "panels": [
      {
        "title": "æ—¥å¿—çº§åˆ«åˆ†å¸ƒ",
        "type": "pie",
        "query": {
          "bool": {
            "must": [
              {
                "range": {
                  "@timestamp": {
                    "gte": "now-24h",
                    "lte": "now"
                  }
                }
              }
            ]
          }
        },
        "aggs": {
          "levels": {
            "terms": {
              "field": "level.keyword"
            }
          }
        }
      },
      {
        "title": "é”™è¯¯è¶‹åŠ¿",
        "type": "line",
        "query": {
          "bool": {
            "must": [
              {
                "term": {
                  "level.keyword": "ERROR"
                }
              },
              {
                "range": {
                  "@timestamp": {
                    "gte": "now-7d",
                    "lte": "now"
                  }
                }
              }
            ]
          }
        },
        "aggs": {
          "timeline": {
            "date_histogram": {
              "field": "@timestamp",
              "interval": "1h"
            }
          }
        }
      },
      {
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "histogram",
        "query": {
          "exists": {
            "field": "response_time"
          }
        },
        "aggs": {
          "response_times": {
            "histogram": {
              "field": "response_time",
              "interval": 100
            }
          }
        }
      }
    ]
  }
}
```

## å‘Šè­¦é…ç½®

### 1. Prometheus å‘Šè­¦è§„åˆ™

```yaml
# alert_rules.yml
groups:
  - name: ontology-framework-alerts
    rules:
      # ç³»ç»Ÿèµ„æºå‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "é«˜ CPU ä½¿ç”¨ç‡å‘Šè­¦"
          description: "å®ä¾‹ {{ $labels.instance }} CPU ä½¿ç”¨ç‡è¶…è¿‡ 80%ï¼Œå½“å‰å€¼: {{ $value }}%"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "é«˜å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦"
          description: "å®ä¾‹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 85%ï¼Œå½“å‰å€¼: {{ $value }}%"

      - alert: DiskSpaceRunningOut
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
          service: ontology-framework
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³å‘Šè­¦"
          description: "å®ä¾‹ {{ $labels.instance }} ç£ç›˜å¯ç”¨ç©ºé—´ä½äº 10%ï¼Œå½“å‰å€¼: {{ $value }}%"

      # åº”ç”¨æ€§èƒ½å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(ontology_object_operations_total{status="error"}[5m]) / rate(ontology_object_operations_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
          description: "åº”ç”¨é”™è¯¯ç‡è¶…è¿‡ 5%ï¼Œå½“å‰å€¼: {{ $value | humanizePercentage }}"

      - alert: SlowQueries
        expr: histogram_quantile(0.95, rate(ontology_query_duration_seconds_bucket[5m])) > 2.0
        for: 5m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "æŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿"
          description: "95% çš„æŸ¥è¯¢å“åº”æ—¶é—´è¶…è¿‡ 2 ç§’ï¼Œå½“å‰å€¼: {{ $value }}s"

      - alert: ServiceDown
        expr: up{job="ontology-framework"} == 0
        for: 1m
        labels:
          severity: critical
          service: ontology-framework
        annotations:
          summary: "æœåŠ¡ä¸å¯ç”¨"
          description: "Ontology Framework æœåŠ¡å®ä¾‹ {{ $labels.instance }} æ— æ³•è®¿é—®"

      # ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦
      - alert: LowCacheHitRate
        expr: ontology_cache_hit_rate < 0.7
        for: 10m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½"
          description: "ç¼“å­˜ {{ $labels.cache_name }} å‘½ä¸­ç‡ä½äº 70%ï¼Œå½“å‰å€¼: {{ $value | humanizePercentage }}"

      - alert: DataIntegrityIssues
        expr: ontology_data_integrity_score < 0.9
        for: 5m
        labels:
          severity: warning
          service: ontology-framework
        annotations:
          summary: "æ•°æ®å®Œæ•´æ€§é—®é¢˜"
          description: "æ•°æ®å®Œæ•´æ€§åˆ†æ•°ä½äº 90%ï¼Œç±»å‹: {{ $labels.validation_type }}ï¼Œå½“å‰å€¼: {{ $value }}"
```

### 2. AlertManager é…ç½®

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'your-password'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  group_by: ['alertname', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m

    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 30m

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://localhost:5001/'

  - name: 'critical-alerts'
    email_configs:
      - to: 'ops-team@company.com,dev-team@company.com'
        subject: '[CRITICAL] Ontology Framework Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          å‘Šè­¦: {{ .Annotations.summary }}
          æè¿°: {{ .Annotations.description }}
          æ—¶é—´: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts-critical'
        title: 'ğŸš¨ Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'warning-alerts'
    email_configs:
      - to: 'dev-team@company.com'
        subject: '[WARNING] Ontology Framework Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          å‘Šè­¦: {{ .Annotations.summary }}
          æè¿°: {{ .Annotations.description }}
          {{ end }}
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts-warning'
        title: 'âš ï¸ Warning Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']
```

## ç›‘æ§å·¥å…·é›†æˆ

### 1. Grafana ä»ªè¡¨æ¿é…ç½®

#### ç³»ç»Ÿç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "Ontology Framework - ç³»ç»Ÿç›‘æ§",
    "tags": ["ontology-framework", "system"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "CPU ä½¿ç”¨ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "{{instance}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 70},
                {"color": "red", "value": 90}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "å†…å­˜ä½¿ç”¨ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "{{instance}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 80},
                {"color": "red", "value": 95}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "ç£ç›˜ä½¿ç”¨ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "(node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes * 100",
            "legendFormat": "{{instance}}: {{mountpoint}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 80},
                {"color": "red", "value": 95}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "30s"
  }
}
```

#### åº”ç”¨æ€§èƒ½ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "Ontology Framework - åº”ç”¨æ€§èƒ½",
    "tags": ["ontology-framework", "application"],
    "panels": [
      {
        "id": 1,
        "title": "è¯·æ±‚é€Ÿç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ontology_object_operations_total[5m])",
            "legendFormat": "{{operation}} - {{object_type}}"
          }
        ],
        "yAxes": [
          {
            "unit": "reqps"
          }
        ],
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "é”™è¯¯ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ontology_object_operations_total{status=\"error\"}[5m]) / rate(ontology_object_operations_total[5m]) * 100",
            "legendFormat": "é”™è¯¯ç‡"
          }
        ],
        "yAxes": [
          {
            "unit": "percent",
            "max": 100,
            "min": 0
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 3,
        "title": "å¹³å‡å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(ontology_query_duration_seconds_bucket[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(ontology_query_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(ontology_query_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ],
        "yAxes": [
          {
            "unit": "s"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ]
  }
}
```

### 2. ä¸šåŠ¡ç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Ontology Framework - ä¸šåŠ¡æŒ‡æ ‡",
    "panels": [
      {
        "title": "æ´»è·ƒç”¨æˆ·æ•°",
        "type": "stat",
        "targets": [
          {
            "expr": "ontology_daily_active_users"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short"
          }
        }
      },
      {
        "title": "å¯¹è±¡æ•°é‡ç»Ÿè®¡",
        "type": "pie",
        "targets": [
          {
            "expr": "ontology_objects_by_type",
            "legendFormat": "{{type}}"
          }
        ]
      },
      {
        "title": "API è¯·æ±‚è¶‹åŠ¿",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ontology_api_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "æ•°æ®å®Œæ•´æ€§åˆ†æ•°",
        "type": "stat",
        "targets": [
          {
            "expr": "ontology_data_integrity_score",
            "legendFormat": "{{validation_type}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "max": 100,
            "min": 0,
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 80},
                {"color": "green", "value": 95}
              ]
            }
          }
        }
      }
    ]
  }
}
```

## æ•…éšœæ’æŸ¥

### 1. å¸¸è§é—®é¢˜è¯Šæ–­

#### æ€§èƒ½é—®é¢˜æ’æŸ¥æµç¨‹

```mermaid
graph TD
    A[æ”¶åˆ°æ€§èƒ½å‘Šè­¦] --> B[ç¡®è®¤å‘Šè­¦èŒƒå›´]
    B --> C{ç³»ç»Ÿçº§åˆ«é—®é¢˜?}
    C -->|æ˜¯| D[æ£€æŸ¥ç³»ç»Ÿèµ„æº]
    C -->|å¦| E[æ£€æŸ¥åº”ç”¨æŒ‡æ ‡]

    D --> D1[CPU ä½¿ç”¨ç‡]
    D --> D2[å†…å­˜ä½¿ç”¨ç‡]
    D --> D3[ç£ç›˜ I/O]
    D --> D4[ç½‘ç»œçŠ¶å†µ]

    E --> E1[å“åº”æ—¶é—´]
    E --> E2[é”™è¯¯ç‡]
    E --> E3[ååé‡]
    E --> E4[èµ„æºåˆ©ç”¨ç‡]

    D1 --> F[åˆ†ææ ¹å› ]
    D2 --> F
    D3 --> F
    D4 --> F
    E1 --> F
    E2 --> F
    E3 --> F
    E4 --> F

    F --> G[åˆ¶å®šè§£å†³æ–¹æ¡ˆ]
    G --> H[å®æ–½ä¿®å¤]
    H --> I[éªŒè¯æ•ˆæœ]
    I --> J[æ–‡æ¡£è®°å½•]
```

#### è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# diagnose_ontology.sh - Ontology Framework è¯Šæ–­è„šæœ¬

echo "ğŸ” Ontology Framework è¯Šæ–­å¼€å§‹"
echo "=================================="

# ç³»ç»Ÿä¿¡æ¯æ”¶é›†
echo "ğŸ“Š ç³»ç»Ÿä¿¡æ¯:"
echo "- ä¸»æœºå: $(hostname)"
echo "- æ“ä½œç³»ç»Ÿ: $(uname -s) $(uname -r)"
echo "- å½“å‰æ—¶é—´: $(date)"
echo "- è¿è¡Œæ—¶é—´: $(uptime -p)"
echo ""

# èµ„æºä½¿ç”¨æƒ…å†µ
echo "ğŸ’» èµ„æºä½¿ç”¨æƒ…å†µ:"
echo "- CPU ä½¿ç”¨ç‡: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)%"
echo "- å†…å­˜ä½¿ç”¨: $(free -h | awk 'NR==2{printf "%.1f%% (%s/%s)", $3*100/$2, $3, $2}')"
echo "- ç£ç›˜ä½¿ç”¨: $(df -h / | awk 'NR==2{print $5 " (" $3 "/" $2 ")"}')"
echo "- è´Ÿè½½å¹³å‡å€¼: $(uptime | awk -F'load average:' '{print $2}')"
echo ""

# æ£€æŸ¥å…³é”®è¿›ç¨‹
echo "ğŸ”§ è¿›ç¨‹çŠ¶æ€:"
echo "- Ontology Framework è¿›ç¨‹:"
pgrep -f "ontology-framework" | while read pid; do
    echo "  PID: $pid, CPU: $(ps -p $pid -o %cpu --no-headers), å†…å­˜: $(ps -p $pid -o %mem --no-headers)"
done
echo ""

# æ£€æŸ¥ç«¯å£çŠ¶æ€
echo "ğŸŒ ç½‘ç»œçŠ¶æ€:"
echo "- æœåŠ¡ç«¯å£æ£€æŸ¥:"
for port in 8080 9090 3000; do
    if netstat -tuln | grep -q ":$port "; then
        echo "  âœ… ç«¯å£ $port æ­£åœ¨ç›‘å¬"
    else
        echo "  âŒ ç«¯å£ $port æœªç›‘å¬"
    fi
done
echo ""

# æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
echo "ğŸ“ æ—¥å¿—åˆ†æ:"
LOG_DIR="/var/log/ontology-framework"
if [ -d "$LOG_DIR" ]; then
    echo "- æœ€è¿‘çš„é”™è¯¯æ—¥å¿—:"
    find "$LOG_DIR" -name "*.log" -mtime -1 -exec grep -l "ERROR\|FATAL" {} \; | head -5 | while read file; do
        echo "  æ–‡ä»¶: $file"
        grep "ERROR\|FATAL" "$file" | tail -3 | sed 's/^/    /'
    done
else
    echo "  æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: $LOG_DIR"
fi
echo ""

# æ£€æŸ¥æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
echo "ğŸ—„ï¸  æ•°æ®åº“çŠ¶æ€:"
if command -v pg_isready &> /dev/null; then
    if pg_isready -h localhost -p 5432; then
        echo "  âœ… PostgreSQL è¿æ¥æ­£å¸¸"
    else
        echo "  âŒ PostgreSQL è¿æ¥å¤±è´¥"
    fi
fi

# æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥
echo "ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:"
if curl -s http://localhost:8080/metrics > /dev/null; then
    echo "  âœ… Prometheus æŒ‡æ ‡ç«¯ç‚¹å¯è®¿é—®"
    # æå–ä¸€äº›å…³é”®æŒ‡æ ‡
    echo "  - å½“å‰æ´»è·ƒä¼šè¯: $(curl -s http://localhost:8080/metrics | grep 'ontology_active_sessions' | cut -d' ' -f2)"
else
    echo "  âŒ æ— æ³•è®¿é—®æŒ‡æ ‡ç«¯ç‚¹"
fi

echo ""
echo "ğŸ” è¯Šæ–­å®Œæˆ"
echo "=================================="
```

#### é—®é¢˜æ’æŸ¥æ£€æŸ¥æ¸…å•

```markdown
## æ€§èƒ½é—®é¢˜æ’æŸ¥æ¸…å•

### ç³»ç»Ÿå±‚é¢
- [ ] CPU ä½¿ç”¨ç‡æ˜¯å¦è¿‡é«˜ (>80%)
- [ ] å†…å­˜ä½¿ç”¨ç‡æ˜¯å¦è¿‡é«˜ (>85%)
- [ ] ç£ç›˜ç©ºé—´æ˜¯å¦ä¸è¶³ (<10%)
- [ ] ç£ç›˜ I/O æ˜¯å¦æœ‰ç“¶é¢ˆ
- [ ] ç½‘ç»œå¸¦å®½æ˜¯å¦é¥±å’Œ
- [ ] ç³»ç»Ÿè´Ÿè½½æ˜¯å¦æ­£å¸¸

### åº”ç”¨å±‚é¢
- [ ] åº”ç”¨è¿›ç¨‹æ˜¯å¦æ­£å¸¸è¿è¡Œ
- [ ] ç«¯å£æ˜¯å¦æ­£å¸¸ç›‘å¬
- [ ] æ—¥å¿—ä¸­æ˜¯å¦æœ‰å¤§é‡é”™è¯¯
- [ ] æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸
- [ ] ç¼“å­˜å‘½ä¸­ç‡æ˜¯å¦åˆç†
- [ ] å“åº”æ—¶é—´æ˜¯å¦å¼‚å¸¸

### ä¸šåŠ¡å±‚é¢
- [ ] ç”¨æˆ·æ“ä½œæˆåŠŸç‡
- [ ] å…³é”®ä¸šåŠ¡æµç¨‹æ˜¯å¦æ­£å¸¸
- [ ] æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- [ ] ç¬¬ä¸‰æ–¹æœåŠ¡ä¾èµ–çŠ¶æ€
```

### 2. æ•…éšœæ¢å¤æµç¨‹

#### è‡ªåŠ¨åŒ–æ•…éšœæ¢å¤

```python
# fault_recovery.py
import time
import logging
import requests
from typing import Dict, List, Callable

class FaultRecoveryManager:
    """æ•…éšœæ¢å¤ç®¡ç†å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.recovery_actions: Dict[str, List[Callable]] = {
            'high_cpu': [
                self._restart_non_critical_services,
                self._scale_up_resources,
                self._enable_caching
            ],
            'high_memory': [
                self._clear_caches,
                self._restart_memory_intensive_services,
                self._increase_memory_limits
            ],
            'service_down': [
                self._restart_service,
                self._check_dependencies,
                self._rollback_recent_changes
            ],
            'database_error': [
                self._check_database_connection,
                self._restart_database_connection_pool,
                self._switch_to_read_replica
            ]
        }

    def detect_fault_type(self, metrics: Dict) -> str:
        """æ£€æµ‹æ•…éšœç±»å‹"""
        if metrics.get('cpu_usage', 0) > 80:
            return 'high_cpu'
        elif metrics.get('memory_usage', 0) > 85:
            return 'high_memory'
        elif metrics.get('service_available', True) == False:
            return 'service_down'
        elif metrics.get('database_error_rate', 0) > 0.05:
            return 'database_error'
        else:
            return 'unknown'

    def execute_recovery(self, fault_type: str, metrics: Dict) -> bool:
        """æ‰§è¡Œæ•…éšœæ¢å¤"""
        if fault_type not in self.recovery_actions:
            self.logger.warning(f"æœªçŸ¥æ•…éšœç±»å‹: {fault_type}")
            return False

        self.logger.info(f"å¼€å§‹æ‰§è¡Œæ•…éšœæ¢å¤: {fault_type}")

        for action in self.recovery_actions[fault_type]:
            try:
                self.logger.info(f"æ‰§è¡Œæ¢å¤æ“ä½œ: {action.__name__}")
                success = action(metrics)
                if success:
                    self.logger.info(f"æ¢å¤æ“ä½œæˆåŠŸ: {action.__name__}")
                    return True
                else:
                    self.logger.warning(f"æ¢å¤æ“ä½œå¤±è´¥: {action.__name__}")
            except Exception as e:
                self.logger.error(f"æ¢å¤æ“ä½œå¼‚å¸¸: {action.__name__}, é”™è¯¯: {e}")

        return False

    def _restart_non_critical_services(self, metrics: Dict) -> bool:
        """é‡å¯éå…³é”®æœåŠ¡"""
        try:
            # é‡å¯éå…³é”®æœåŠ¡çš„é€»è¾‘
            services_to_restart = ['analytics', 'reporting', 'background-jobs']
            for service in services_to_restart:
                # è¿™é‡Œè°ƒç”¨å®é™…çš„APIæˆ–å‘½ä»¤é‡å¯æœåŠ¡
                self.logger.info(f"é‡å¯æœåŠ¡: {service}")
            return True
        except Exception as e:
            self.logger.error(f"é‡å¯æœåŠ¡å¤±è´¥: {e}")
            return False

    def _clear_caches(self, metrics: Dict) -> bool:
        """æ¸…ç†ç¼“å­˜"""
        try:
            # è°ƒç”¨ç¼“å­˜æ¸…ç†API
            response = requests.post('http://localhost:8080/api/admin/cache/clear')
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            return False

    def _restart_service(self, metrics: Dict) -> bool:
        """é‡å¯æœåŠ¡"""
        try:
            # é‡å¯ä¸»åº”ç”¨æœåŠ¡
            import subprocess
            result = subprocess.run(['systemctl', 'restart', 'ontology-framework'],
                                 capture_output=True, text=True)
            return result.returncode == 0
        except Exception as e:
            self.logger.error(f"é‡å¯æœåŠ¡å¤±è´¥: {e}")
            return False

# è‡ªåŠ¨åŒ–ç›‘æ§å’Œæ¢å¤
class AutomatedMonitoring:
    """è‡ªåŠ¨åŒ–ç›‘æ§å’Œæ¢å¤"""

    def __init__(self):
        self.recovery_manager = FaultRecoveryManager()
        self.monitoring_interval = 60  # 60ç§’æ£€æŸ¥ä¸€æ¬¡
        self.recovery_cooldown = 300  # 5åˆ†é’Ÿæ¢å¤å†·å´æ—¶é—´
        self.last_recovery_time = {}

    def monitor_and_recover(self):
        """ç›‘æ§å’Œæ¢å¤å¾ªç¯"""
        while True:
            try:
                # æ”¶é›†æŒ‡æ ‡
                metrics = self.collect_metrics()

                # æ£€æµ‹æ•…éšœ
                fault_type = self.recovery_manager.detect_fault_type(metrics)

                if fault_type != 'unknown':
                    # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
                    if self._can_execute_recovery(fault_type):
                        self.logger.info(f"æ£€æµ‹åˆ°æ•…éšœ: {fault_type}")

                        # æ‰§è¡Œæ¢å¤
                        success = self.recovery_manager.execute_recovery(fault_type, metrics)

                        if success:
                            self.logger.info(f"æ•…éšœæ¢å¤æˆåŠŸ: {fault_type}")
                            self.last_recovery_time[fault_type] = time.time()
                        else:
                            self.logger.error(f"æ•…éšœæ¢å¤å¤±è´¥: {fault_type}")
                            # å‘é€å‘Šè­¦é€šçŸ¥
                            self.send_alert(fault_type, metrics)

                time.sleep(self.monitoring_interval)

            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(30)

    def _can_execute_recovery(self, fault_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œæ¢å¤"""
        if fault_type not in self.last_recovery_time:
            return True

        time_since_last = time.time() - self.last_recovery_time[fault_type]
        return time_since_last >= self.recovery_cooldown

    def collect_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æŒ‡æ ‡æ”¶é›†é€»è¾‘
        return {
            'cpu_usage': 75.5,  # ç¤ºä¾‹æ•°æ®
            'memory_usage': 82.3,
            'service_available': True,
            'database_error_rate': 0.01
        }

    def send_alert(self, fault_type: str, metrics: Dict):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        # å®ç°å‘Šè­¦é€šçŸ¥é€»è¾‘
        self.logger.critical(f"éœ€è¦äººå·¥å¹²é¢„: {fault_type}, æŒ‡æ ‡: {metrics}")

# å¯åŠ¨è‡ªåŠ¨åŒ–ç›‘æ§
if __name__ == "__main__":
    monitoring = AutomatedMonitoring()
    monitoring.monitor_and_recover()
```

## ç›‘æ§æœ€ä½³å®è·µ

### 1. ç›‘æ§ç­–ç•¥

#### å››ä¸ªé»„é‡‘æŒ‡æ ‡

```yaml
ç›‘æ§é»„é‡‘æŒ‡æ ‡:
  å»¶è¿Ÿ (Latency):
    - è¯·æ±‚å“åº”æ—¶é—´
    - å¤„ç†æ—¶é—´åˆ†å¸ƒ
    - P50, P95, P99 å»¶è¿Ÿ

  æµé‡ (Traffic):
    - æ¯ç§’è¯·æ±‚æ•° (QPS)
    - æ•°æ®ä¼ è¾“é‡
    - å¹¶å‘è¿æ¥æ•°

  é”™è¯¯ (Errors):
    - é”™è¯¯ç‡
    - HTTP çŠ¶æ€ç åˆ†å¸ƒ
    - å¼‚å¸¸æ•°é‡

  é¥±å’Œåº¦ (Saturation):
    - CPU ä½¿ç”¨ç‡
    - å†…å­˜ä½¿ç”¨ç‡
    - ç£ç›˜ä½¿ç”¨ç‡
    - è¿æ¥æ± ä½¿ç”¨ç‡
```

#### ç›‘æ§åˆ†å±‚

```mermaid
graph TB
    L1[åŸºç¡€è®¾æ–½å±‚] --> L2[å¹³å°å±‚]
    L2 --> L3[åº”ç”¨å±‚]
    L3 --> L4[ä¸šåŠ¡å±‚]
    L4 --> L5[ç”¨æˆ·ä½“éªŒå±‚]

    subgraph "åŸºç¡€è®¾æ–½å±‚ç›‘æ§"
        A1[æœåŠ¡å™¨èµ„æº]
        A2[ç½‘ç»œè®¾å¤‡]
        A3[å­˜å‚¨ç³»ç»Ÿ]
    end

    subgraph "å¹³å°å±‚ç›‘æ§"
        B1[å®¹å™¨å¹³å°]
        B2[æ•°æ®åº“]
        B3[æ¶ˆæ¯é˜Ÿåˆ—]
    end

    subgraph "åº”ç”¨å±‚ç›‘æ§"
        C1[åº”ç”¨æ€§èƒ½]
        C2[ä¾èµ–æœåŠ¡]
        C3[é”™è¯¯ç›‘æ§]
    end

    subgraph "ä¸šåŠ¡å±‚ç›‘æ§"
        D1[ä¸šåŠ¡æŒ‡æ ‡]
        D2[ç”¨æˆ·è¡Œä¸º]
        D3[æ•°æ®è´¨é‡]
    end

    subgraph "ç”¨æˆ·ä½“éªŒå±‚ç›‘æ§"
        E1[é¡µé¢æ€§èƒ½]
        E2[ç”¨æˆ·äº¤äº’]
        E3[å¯ç”¨æ€§]
    end
```

### 2. ç›‘æ§é…ç½®æœ€ä½³å®è·µ

#### æŒ‡æ ‡å‘½åè§„èŒƒ

```python
# æŒ‡æ ‡å‘½åè§„èŒƒ
"""
æŒ‡æ ‡å‘½åéµå¾ªä»¥ä¸‹è§„èŒƒï¼š
1. ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”çš„å°å†™å­—æ¯
2. åŒ…å«åº”ç”¨åç§°å‰ç¼€
3. æè¿°æŒ‡æ ‡çš„å…·ä½“å«ä¹‰
4. åŒ…å«å•ä½åç¼€ï¼ˆå¯é€‰ï¼‰

ç¤ºä¾‹ï¼š
- ontology_object_operations_total
- ontology_query_duration_seconds
- ontology_cache_hit_rate
- ontology_active_users
"""

# æ­£ç¡®çš„æŒ‡æ ‡å®šä¹‰
REQUEST_COUNT = Counter(
    'ontology_http_requests_total',  # åº”ç”¨å‰ç¼€ + æŒ‡æ ‡åç§° + å•ä½
    'Total number of HTTP requests',   # æè¿°
    ['method', 'endpoint', 'status']  # æ ‡ç­¾
)

RESPONSE_TIME = Histogram(
    'ontology_http_request_duration_seconds',  # å•ä½æ˜ç¡®
    'HTTP request duration in seconds',
    ['method', 'endpoint'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # åˆç†çš„æ¡¶è®¾ç½®
)
```

#### å‘Šè­¦ç­–ç•¥

```yaml
# å‘Šè­¦åˆ†çº§ç­–ç•¥
å‘Šè­¦çº§åˆ«:
  Critical (ç´§æ€¥):
    - æœåŠ¡å®Œå…¨ä¸å¯ç”¨
    - æ•°æ®ä¸¢å¤±é£é™©
    - å®‰å…¨æ¼æ´
    - å“åº”æ—¶é—´: é˜ˆå€¼: ç«‹å³, é€šçŸ¥: å…¨å¤©å€™

  Warning (è­¦å‘Š):
    - æ€§èƒ½ä¸‹é™
    - èµ„æºä½¿ç”¨ç‡é«˜
    - é”™è¯¯ç‡ä¸Šå‡
    - å“åº”æ—¶é—´: é˜ˆå€¼: 5åˆ†é’Ÿ, é€šçŸ¥: å·¥ä½œæ—¶é—´

  Info (ä¿¡æ¯):
    - é…ç½®å˜æ›´
    - éƒ¨ç½²å®Œæˆ
    - æ€§èƒ½ä¼˜åŒ–å»ºè®®
    - å“åº”æ—¶é—´: é˜ˆå€¼: å®šæœŸæŠ¥å‘Š
```

### 3. è¿ç»´è‡ªåŠ¨åŒ–

#### è‡ªåŠ¨åŒ–æ‰©ç¼©å®¹

```yaml
# HPA (Horizontal Pod Autoscaler) é…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ontology-framework-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ontology-framework
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: ontology_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
```

#### è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå›æ»š

```yaml
# GitOps è‡ªåŠ¨åŒ–éƒ¨ç½²
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ontology-framework
spec:
  project: default
  source:
    repoURL: https://github.com/company/ontology-framework
    targetRevision: HEAD
    path: k8s/production
  destination:
    server: https://kubernetes.default.svc
    namespace: ontology-framework
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
  hooks:
    - type: Sync
      sync:
        - name: pre-deploy-check
          command: ["sh", "-c", "kubectl get pods -n ontology-framework"]
        - name: post-deploy-verify
          command: ["sh", "-c", "curl -f http://ontology-framework.ontology-framework.svc.cluster.local/health"]
```

### 4. ç›‘æ§æ²»ç†

#### ç›‘æ§æŒ‡æ ‡æ²»ç†

```python
# ç›‘æ§æŒ‡æ ‡æ³¨å†Œå’Œç®¡ç†
class MetricsRegistry:
    """ç›‘æ§æŒ‡æ ‡æ³¨å†Œä¸­å¿ƒ"""

    def __init__(self):
        self.metrics = {}
        self.metric_schemas = {}

    def register_metric(self, name: str, metric_type: str, description: str,
                       labels: List[str] = None, unit: str = None):
        """æ³¨å†Œç›‘æ§æŒ‡æ ‡"""
        if name in self.metrics:
            raise ValueError(f"æŒ‡æ ‡ {name} å·²å­˜åœ¨")

        # éªŒè¯æŒ‡æ ‡åç§°
        if not self._validate_metric_name(name):
            raise ValueError(f"æŒ‡æ ‡åç§° {name} ä¸ç¬¦åˆè§„èŒƒ")

        self.metrics[name] = {
            'type': metric_type,
            'description': description,
            'labels': labels or [],
            'unit': unit,
            'created_at': time.time(),
            'created_by': self._get_caller()
        }

        self.metric_schemas[name] = self._generate_schema(name, metric_type, labels)

    def _validate_metric_name(self, name: str) -> bool:
        """éªŒè¯æŒ‡æ ‡åç§°è§„èŒƒ"""
        import re
        pattern = r'^[a-z][a-z0-9_]*(_total|_seconds|_bytes|_ratio|_percent)?$'
        return bool(re.match(pattern, name))

    def list_metrics(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ³¨å†Œçš„æŒ‡æ ‡"""
        return list(self.metrics.keys())

    def get_metric_info(self, name: str) -> Dict:
        """è·å–æŒ‡æ ‡è¯¦ç»†ä¿¡æ¯"""
        return self.metrics.get(name, {})

    def export_prometheus_config(self) -> str:
        """å¯¼å‡º Prometheus é…ç½®"""
        config = "# è‡ªåŠ¨ç”Ÿæˆçš„ç›‘æ§æŒ‡æ ‡é…ç½®\n\n"
        for name, info in self.metrics.items():
            config += f"# {name}\n"
            config += f"# {info['description']}\n"
            if info['labels']:
                config += f"# Labels: {', '.join(info['labels'])}\n"
            if info['unit']:
                config += f"# Unit: {info['unit']}\n"
            config += "\n"

        return config

# å…¨å±€æŒ‡æ ‡æ³¨å†Œå™¨
metrics_registry = MetricsRegistry()

# ä½¿ç”¨è£…é¥°å™¨è‡ªåŠ¨æ³¨å†ŒæŒ‡æ ‡
def register_metric(name: str, metric_type: str = 'counter', **kwargs):
    """æŒ‡æ ‡æ³¨å†Œè£…é¥°å™¨"""
    def decorator(func):
        # è‡ªåŠ¨æ³¨å†ŒæŒ‡æ ‡
        metrics_registry.register_metric(
            name=name,
            metric_type=metric_type,
            description=f"è‡ªåŠ¨æ³¨å†Œçš„æŒ‡æ ‡: {func.__name__}",
            **kwargs
        )

        def wrapper(*args, **kwargs):
            # åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æŒ‡æ ‡è®°å½•é€»è¾‘
            return func(*args, **kwargs)

        return wrapper
    return decorator
```

---

## æ€»ç»“

æœ¬æŒ‡å—æä¾›äº† Ontology Framework ç”Ÿäº§ç¯å¢ƒç›‘æ§çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

### æ ¸å¿ƒåŠŸèƒ½
- âœ… å¤šå±‚æ¬¡ç›‘æ§æ¶æ„
- âœ… å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- âœ… ç»“æ„åŒ–æ—¥å¿—ç®¡ç†
- âœ… æ™ºèƒ½å‘Šè­¦é…ç½®
- âœ… è‡ªåŠ¨æ•…éšœæ¢å¤
- âœ… å¯è§†åŒ–ä»ªè¡¨æ¿

### æœ€ä½³å®è·µ
- ğŸ“Š éµå¾ªå››ä¸ªé»„é‡‘æŒ‡æ ‡ç›‘æ§åŸåˆ™
- ğŸ¯ åˆ†å±‚ç›‘æ§ç­–ç•¥
- ğŸ”§ è‡ªåŠ¨åŒ–è¿ç»´æµç¨‹
- ğŸ“ˆ æŒç»­æ€§èƒ½ä¼˜åŒ–
- ğŸ›¡ï¸ å®‰å…¨ç›‘æ§è€ƒè™‘

### å®æ–½ä»·å€¼
- **æé«˜ç³»ç»Ÿå¯é æ€§**: ä¸»åŠ¨å‘ç°é—®é¢˜ï¼Œå¿«é€Ÿå“åº”æ•…éšœ
- **ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ**: å®æ—¶ç›‘æ§åº”ç”¨æ€§èƒ½ï¼Œç¡®ä¿æœåŠ¡è´¨é‡
- **é™ä½è¿ç»´æˆæœ¬**: è‡ªåŠ¨åŒ–ç›‘æ§å’Œå‘Šè­¦ï¼Œå‡å°‘äººå·¥å¹²é¢„
- **æ”¯æŒä¸šåŠ¡å†³ç­–**: æ•°æ®é©±åŠ¨çš„è¿ç»´å†³ç­–

é€šè¿‡å®æ–½æœ¬æŒ‡å—çš„ç›‘æ§æ–¹æ¡ˆï¼Œå¯ä»¥ç¡®ä¿ Ontology Framework åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¨³å®šã€é«˜æ•ˆåœ°è¿è¡Œï¼Œå¹¶ä¸ºæŒç»­çš„æ€§èƒ½ä¼˜åŒ–æä¾›æ•°æ®æ”¯æŒã€‚